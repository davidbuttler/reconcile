/*
 *    This program is free software; you can redistribute it and/or modify
 *    it under the terms of the GNU General Public License as published by
 *    the Free Software Foundation; either version 2 of the License, or
 *    (at your option) any later version.
 *
 *    This program is distributed in the hope that it will be useful,
 *    but WITHOUT ANY WARRANTY; without even the implied warranty of
 *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *    GNU General Public License for more details.
 *
 *    You should have received a copy of the GNU General Public License
 *    along with this program; if not, write to the Free Software
 *    Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 */

/*
 *    JRip.java
 *    Copyright (C) 2001 Xin Xu, Eibe Frank
 */

package reconcile.weka.classifiers.rules;

import java.io.Serializable;
import java.text.NumberFormat;
import java.util.Enumeration;
import java.util.Random;
import java.util.Vector;

import reconcile.weka.classifiers.Classifier;
import reconcile.weka.classifiers.Evaluation;
import reconcile.weka.core.AdditionalMeasureProducer;
import reconcile.weka.core.Attribute;
import reconcile.weka.core.Copyable;
import reconcile.weka.core.FastVector;
import reconcile.weka.core.Instance;
import reconcile.weka.core.Instances;
import reconcile.weka.core.ModifiedInstances;
import reconcile.weka.core.Option;
import reconcile.weka.core.OptionHandler;
import reconcile.weka.core.UnsupportedAttributeTypeException;
import reconcile.weka.core.UnsupportedClassTypeException;
import reconcile.weka.core.Utils;
import reconcile.weka.core.WeightedInstancesHandler;
import reconcile.weka.filters.Filter;


/**
 * This class implements a propositional rule learner, Repeated Incremental
 * Pruning to Produce Error Reduction (RIPPER), which is proposed by William W.
 * Cohen as an optimized version of IREP.
 * <p>
 * 
 * The algorithm is briefly described as follows:
 * <p>
 * Initialize RS = {}, and for each class from the less prevalent one to the
 * more frequent one, DO:
 * <p>
 * 
 * 1. Building stage: repeat 1.1 and 1.2 until the descrition length (DL) of the
 * ruleset and examples is 64 bits greater than the smallest DL met so far, or
 * there are no positive examples, or the error rate >= 50%.
 * <p>
 * 1.1. Grow phase:<br>
 * Grow one rule by greedily adding antecedents (or conditions) to the rule
 * until the rule is perfect (i.e. 100% accurate). The procedure tries every
 * possible value of each attribute and selects the condition with highest
 * information gain: p(log(p/t)-log(P/T)).
 * <p>
 * 1.2. Prune phase:<br>
 * Incrementally prune each rule and allow the pruning of any final sequences of
 * the antecedents;<br>
 * The pruning metric is (p-n)/(p+n) -- but it's actually 2p/(p+n) -1, so in
 * this implementation we simply use p/(p+n) (actually (p+1)/(p+n+2), thus if
 * p+n is 0, it's 0.5).
 * <p>
 * 
 * 2. Optimization stage: after generating the initial ruleset {Ri}, generate
 * and prune two variants of each rule Ri from randomized data using procedure
 * 1.1 and 1.2. But one variant is generated from an empty rule while the other
 * is generated by greedily adding antecedents to the original rule. Moreover,
 * the pruning metric used here is (TP+TN)/(P+N).<br>
 * Then the smallest possible DL for each variant and the original rule is
 * computed. The variant with the minimal DL is selected as the final
 * representative of Ri in the ruleset. <br>
 * After all the rules in {Ri} have been examined and if there are still
 * residual positives, more rules are generated based on the residual positives
 * using Building Stage again.
 * <p>
 * 
 * 3. Delete the rules from the ruleset that would increase the DL of the whole
 * ruleset if it were in it. and add resultant ruleset to RS.
 * <p>
 * 
 * ENDDO
 * <p>
 * 
 * Note that there seem to be 2 bugs in the ripper program that would affect the
 * ruleset size and accuracy slightly. This implementation avoids these bugs and
 * thus is a little bit different from Cohen's original implementation. Even
 * after fixing the bugs, since the order of classes with the same frequency is
 * not defined in ripper, there still seems to be some trivial difference
 * between this implementation and the original ripper, especially for audiology
 * data in UCI repository, where there are lots of classes of few instances.
 * <p>
 * 
 * If wrapped by other classes, typical usage of this class is:<br>
 * 
 * <code>JRip rip = new JRip();
 * Instances data = ... // Data from somewhere
 * double[] orderedClasses = ... // Get the ordered class counts for the data 
 * double expFPRate = ... // Calculate the expected FP/(FP+FN) rate
 * double classIndex = ...  // The class index for which ruleset is built
 * // DL of default rule, no theory DL, only data DL
 * double defDL = StRuleStats.dataDL(expFPRate, 0.0, data.sumOfWeights(),
 *				   0.0, orderedClasses[(int)classIndex]);
 *	    
 * rip.rulesetForOneClass(expFPRate, data, classIndex, defDL); 
 * StRuleStats rulesetStats = rip.getRuleStats(0);
 *
 * // Can get heaps of information from StRuleStats, e.g. combined DL, 
 * // simpleStats, etc.
 * double comDL = rulesetStats.combinedDL(expFPRate, classIndex);
 * int whichRule = ... // Want simple stats of which rule?
 * double[] simpleStats = rulesetStats.getSimpleStats(whichRule);
 * ...
 * </code>
 * 
 * Details please see "Fast Effective Rule Induction", William W. Cohen,
 * 'Machine Learning: Proceedings of the Twelfth International Conference'
 * (ML95).
 * <p>
 * 
 * PS. We have compared this implementation with the original ripper
 * implementation in aspects of accuracy, ruleset size and running time on both
 * artificial data "ab+bcd+defg" and UCI datasets. In all these aspects it seems
 * to be quite comparable to the original ripper implementation. However, we
 * didn't consider memory consumption optimization in this implementation.
 * <p>
 * 
 * @author Xin Xu (xx5@cs.waikato.ac.nz)
 * @author Eibe Frank (eibe@cs.waikato.ac.nz)
 * @version $Revision: 1.1 $
 */
/**
 * @author ves
 * 
 * TODO To change the template for this generated type comment go to Window -
 * Preferences - Java - Code Style - Code Templates
 */
/**
 * @author ves
 * 
 * TODO To change the template for this generated type comment go to Window -
 * Preferences - Java - Code Style - Code Templates
 */
public class StRip extends Classifier implements OptionHandler,
		AdditionalMeasureProducer, WeightedInstancesHandler {

	/** The limit of description length surplus in ruleset generation */
	private static double MAX_DL_SURPLUS = 64.0;

	/** The class attribute of the data */
	private Attribute m_Class;

	/** The real class attribute of the data needed for processing */
	private Attribute m_RealClass;

	/** The predicted class attribute of the data needed for processing */
	private Attribute m_PredictedClass;

	/** The covered attribute of the data needed for processing */
	private Attribute m_Covered;

	/** The ruleset */
	private FastVector m_Ruleset;

	/** The predicted class distribution */
	private FastVector m_Distributions;

	/** Runs of optimizations */
	private int m_Optimizations = 2;
	
	/** Ratio of weight pos/neg for examles */
	private float m_Ratio = 1;


	/** Random object used in this class */
	private Random m_Random = null;

	/** # of all the possible conditions in a rule */
	private double m_Total = 0;

	/** The seed to perform randomization */
	private long m_Seed = 1;

	/** The number of folds to split data into Grow and Prune for IREP */
	private int m_Folds = 3;

	/** The minimal number of instance weights within a split */
	private double m_MinNo = 2.0;

	/** Whether in a debug mode */
	private boolean m_Debug = true;

	/** An additional debug mode (including even more information */
	private boolean ves_Debug = false;

	/** Whether check the error rate >= 0.5 in stopping criteria */
	private boolean m_CheckErr = true;

	/** Whether use pruning, i.e. the data is clean or not */
	private boolean m_UsePruning = true;

	/** The filter used to randomize the class order */
	private Filter m_Filter = null;

	/** Used to store the instances */
	private Instances m_Instances = null;

	/** The RuleStats for the ruleset of each class value */
	private FastVector m_RulesetStats;
	
	/** The number of distinct values allowed for numeric attributes*/
	private int m_MaxNumAttributes = 100;

	/**
	 * Returns a string describing classifier
	 * 
	 * @return a description suitable for displaying in the explorer/experimenter
	 *         gui
	 */
	public String globalInfo() {

		return "This class implements a propositional rule learner, Repeated Incremental "
				+ "Pruning to Produce Error Reduction (RIPPER), which was proposed by William "
				+ "W. Cohen as an optimized version of IREP. \n\n"
				+ "The algorithm is briefly described as follows: \n\n"
				+ "Initialize RS = {}, and for each class from the less prevalent one to "
				+ "the more frequent one, DO: \n\n"
				+ "1. Building stage:\nRepeat 1.1 and 1.2 until the descrition length (DL) "
				+ "of the ruleset and examples is 64 bits greater than the smallest DL "
				+ "met so far, or there are no positive examples, or the error rate >= 50%. "
				+ "\n\n"
				+ "1.1. Grow phase:\n"
				+ "Grow one rule by greedily adding antecedents (or conditions) to "
				+ "the rule until the rule is perfect (i.e. 100% accurate).  The "
				+ "procedure tries every possible value of each attribute and selects "
				+ "the condition with highest information gain: p(log(p/t)-log(P/T))."
				+ "\n\n"
				+ "1.2. Prune phase:\n"
				+ "Incrementally prune each rule and allow the pruning of any "
				+ "final sequences of the antecedents;"
				+ "The pruning metric is (p-n)/(p+n) -- but it's actually "
				+ "2p/(p+n) -1, so in this implementation we simply use p/(p+n) "
				+ "(actually (p+1)/(p+n+2), thus if p+n is 0, it's 0.5).\n\n"
				+ "2. Optimization stage:\n after generating the initial ruleset {Ri}, "
				+ "generate and prune two variants of each rule Ri from randomized data "
				+ "using procedure 1.1 and 1.2. But one variant is generated from an "
				+ "empty rule while the other is generated by greedily adding antecedents "
				+ "to the original rule. Moreover, the pruning metric used here is "
				+ "(TP+TN)/(P+N)."
				+ "Then the smallest possible DL for each variant and the original rule "
				+ "is computed.  The variant with the minimal DL is selected as the final "
				+ "representative of Ri in the ruleset."
				+ "After all the rules in {Ri} have been examined and if there are still "
				+ "residual positives, more rules are generated based on the residual "
				+ "positives using Building Stage again. \n"
				+ "3. Delete the rules from the ruleset that would increase the DL of the "
				+ "whole ruleset if it were in it. and add resultant ruleset to RS. \n"
				+ "ENDDO\n\n"
				+ "Note that there seem to be 2 bugs in the original ripper program that would "
				+ "affect the ruleset size and accuracy slightly.  This implementation avoids "
				+ "these bugs and thus is a little bit different from Cohen's original "
				+ "implementation. Even after fixing the bugs, since the order of classes with "
				+ "the same frequency is not defined in ripper, there still seems to be "
				+ "some trivial difference between this implementation and the original ripper, "
				+ "especially for audiology data in UCI repository, where there are lots of "
				+ "classes of few instances.\n\n"
				+ "Details please see \"Fast Effective Rule Induction\", William W. Cohen, "
				+ "'Machine Learning: Proceedings of the Twelfth International Conference'"
				+ "(ML95). \n\n"
				+ "PS.  We have compared this implementation with the original ripper "
				+ "implementation in aspects of accuracy, ruleset size and running time "
				+ "on both artificial data \"ab+bcd+defg\" and UCI datasets.  In all these "
				+ "aspects it seems to be quite comparable to the original ripper "
				+ "implementation.  However, we didn't consider memory consumption "
				+ "optimization in this implementation.\n\n";
	}

	/**
	 * Returns an enumeration describing the available options Valid options are:
	 * <p>
	 * 
	 * -F number <br>
	 * The number of folds for reduced error pruning. One fold is used as the
	 * pruning set. (Default: 3)
	 * <p>
	 * 
	 * -N number <br>
	 * The minimal weights of instances within a split. (Default: 2)
	 * <p>
	 * 
	 * -O number <br>
	 * Set the number of runs of optimizations. (Default: 2)
	 * <p>
	 * 
	 * -D<br>
	 * Whether turn on the debug mode
	 * 
	 * -S number <br>
	 * The seed of randomization used in Ripper.(Default: 1)
	 * <p>
	 * 
	 * -E<br>
	 * Whether NOT check the error rate >= 0.5 in stopping criteria. (default:
	 * check)
	 * <p>
	 * 
	 * -P<br>
	 * Whether NOT use pruning. (default: use pruning)
	 * <p>
	 * 
	 * @return an enumeration of all the available options
	 */
	public Enumeration listOptions() {
		Vector newVector = new Vector(3);
		newVector.addElement(new Option("\tSet number of folds for REP\n"
				+ "\tOne fold is used as pruning set.\n" + "\t(default 3)", "F", 1,
				"-F <number of folds>"));
		newVector.addElement(new Option("\tSet the minimal weights of instances\n"
				+ "\twithin a split.\n" + "\t(default 2.0)", "N", 1,
				"-N <min. weights>"));
		newVector.addElement(new Option("\tSet the number of runs of\n"
				+ "\toptimizations. (Default: 2)", "O", 1, "-O <number of runs>"));
		newVector.addElement(new Option("\tSet the weight ratio pos/neg\n"
			+ "\tof examples. (Default: 1)", "L", 1, "-L <ratio>"));

		newVector.addElement(new Option("\tSet whether turn on the\n"
				+ "\tdebug mode (Default: false)", "D", 0, "-D"));

		newVector.addElement(new Option("\tThe seed of randomization\n"
				+ "\t(Default: 1)", "S", 1, "-S <seed>"));

		newVector.addElement(new Option("Whether NOT check the error rate>=0.5\n"
				+ "\tin stopping criteria " + "\t(default: check)", "E", 0, "-E"));

		newVector.addElement(new Option("Whether NOT use pruning\n"
				+ "\t(default: use pruning)", "P", 0, "-P"));
		return newVector.elements();
	}

	/**
	 * Parses a given list of options.
	 * 
	 * @param options
	 *          the list of options as an array of strings
	 * @exception Exception
	 *              if an option is not supported
	 */
	public void setOptions(String[] options) throws Exception {
		String numFoldsString = Utils.getOption('F', options);
		if (numFoldsString.length() != 0)
			m_Folds = Integer.parseInt(numFoldsString);
		else
			m_Folds = 3;

		String minNoString = Utils.getOption('N', options);
		if (minNoString.length() != 0)
			m_MinNo = Double.parseDouble(minNoString);
		else
			m_MinNo = 2.0;

		String seedString = Utils.getOption('S', options);
		if (seedString.length() != 0)
			m_Seed = Long.parseLong(seedString);
		else
			m_Seed = 1;

		String runString = Utils.getOption('O', options);
		if (runString.length() != 0)
			m_Optimizations = Integer.parseInt(runString);
		else
			m_Optimizations = 2;
		
		String ratioString = Utils.getOption('L', options);
		if (ratioString.length() != 0)
			m_Ratio = Float.parseFloat(ratioString);
		else
			m_Ratio = 1;

		m_Debug = Utils.getFlag('D', options);
		m_CheckErr = !Utils.getFlag('E', options);
		m_UsePruning = !Utils.getFlag('P', options);
	}

	/**
	 * Gets the current settings of the Classifier.
	 * 
	 * @return an array of strings suitable for passing to setOptions
	 */
	public String[] getOptions() {

		String[] options = new String[14];
		int current = 0;
		options[current++] = "-F";
		options[current++] = "" + m_Folds;
		options[current++] = "-N";
		options[current++] = "" + m_MinNo;
		options[current++] = "-O";
		options[current++] = "" + m_Optimizations;
		options[current++] = "-L";
		options[current++] = "" + m_Ratio;
		options[current++] = "-S";
		options[current++] = "" + m_Seed;

		if (m_Debug)
			options[current++] = "-D";

		if (!m_CheckErr)
			options[current++] = "-E";

		if (!m_UsePruning)
			options[current++] = "-P";

		while (current < options.length)
			options[current++] = "";

		return options;
	}

	/**
	 * Returns an enumeration of the additional measure names
	 * 
	 * @return an enumeration of the measure names
	 */
	public Enumeration enumerateMeasures() {
		Vector newVector = new Vector(1);
		newVector.addElement("measureNumRules");
		return newVector.elements();
	}

	/**
	 * Returns the value of the named measure
	 * 
	 * @param measureName
	 *          the name of the measure to query for its value
	 * @return the value of the named measure
	 * @exception IllegalArgumentException
	 *              if the named measure is not supported
	 */
	public double getMeasure(String additionalMeasureName) {
		if (additionalMeasureName.compareToIgnoreCase("measureNumRules") == 0)
			return m_Ruleset.size();
		else
			throw new IllegalArgumentException(additionalMeasureName
					+ " not supported (RIPPER)");
	}

	/**
	 * Returns the tip text for this property
	 * 
	 * @return tip text for this property suitable for displaying in the
	 *         explorer/experimenter gui
	 */
	public String foldsTipText() {
		return "Determines the amount of data used for pruning. One fold is used for "
				+ "pruning, the rest for growing the rules.";
	}

	public void setFolds(int fold) {
		m_Folds = fold;
	}

	public int getFolds() {
		return m_Folds;
	}

	/**
	 * Returns the tip text for this property
	 * 
	 * @return tip text for this property suitable for displaying in the
	 *         explorer/experimenter gui
	 */
	public String minNoTipText() {
		return "The minimum total weight of the instances in a rule.";
	}

	public void setMinNo(double m) {
		m_MinNo = m;
	}

	public double getMinNo() {
		return m_MinNo;
	}

	/**
	 * Returns the tip text for this property
	 * 
	 * @return tip text for this property suitable for displaying in the
	 *         explorer/experimenter gui
	 */
	public String seedTipText() {
		return "The seed used for randomizing the data.";
	}

	public void setSeed(long s) {
		m_Seed = s;
	}

	public long getSeed() {
		return m_Seed;
	}

	/**
	 * Returns the tip text for this property
	 * 
	 * @return tip text for this property suitable for displaying in the
	 *         explorer/experimenter gui
	 */
	public String optimizationsTipText() {
		return "The number of optimization runs.";
	}

	public void setOptimizations(int run) {
		m_Optimizations = run;
	}

	public int getOptimizations() {
		return m_Optimizations;
	}
	
	/**
	 * Returns the tip text for this property
	 * 
	 * @return tip text for this property suitable for displaying in the
	 *         explorer/experimenter gui
	 */
	public String ratioTipText() {
		return "The number of optimization runs.";
	}

	public void setRatio(float run) {
		m_Ratio = run;
	}

	public double getRatio() {
		return m_Ratio;
	}

	/**
	 * Returns the tip text for this property
	 * 
	 * @return tip text for this property suitable for displaying in the
	 *         explorer/experimenter gui
	 */
	public String debugTipText() {
		return "Whether debug information is output to the console.";
	}

	public void setDebug(boolean d) {
		m_Debug = d;
	}

	public boolean getDebug() {
		return m_Debug;
	}

	/**
	 * Returns the tip text for this property
	 * 
	 * @return tip text for this property suitable for displaying in the
	 *         explorer/experimenter gui
	 */
	public String checkErrorRateTipText() {
		return "Whether check for error rate >= 1/2 is included"
				+ " in stopping criterion.";
	}

	public void setCheckErrorRate(boolean d) {
		m_CheckErr = d;
	}

	public boolean getCheckErrorRate() {
		return m_CheckErr;
	}

	/**
	 * Returns the tip text for this property
	 * 
	 * @return tip text for this property suitable for displaying in the
	 *         explorer/experimenter gui
	 */
	public String usePruningTipText() {
		return "Whether pruning is performed.";
	}

	public void setUsePruning(boolean d) {
		m_UsePruning = d;
	}

	public boolean getUsePruning() {
		return m_UsePruning;
	}

	/**
	 * Get the ruleset generated by Ripper
	 * 
	 * @return the ruleset
	 */
	public FastVector getRuleset() {
		return m_Ruleset;
	}

	/**
	 * Get the statistics of the ruleset in the given position
	 * 
	 * @param pos
	 *          the position of the stats, assuming correct
	 */
	public StRuleStats getRuleStats(int pos) {
		return (StRuleStats) m_RulesetStats.elementAt(pos);
	}

	/**
	 * The single antecedent in the rule, which is composed of an attribute and
	 * the corresponding value. There are two inherited classes, namely
	 * NumericAntd and NominalAntd in which the attributes are numeric and nominal
	 * respectively.
	 */
	private abstract class Antd implements WeightedInstancesHandler, Copyable,
			Serializable {

		/* The attribute of the antecedent */
		protected Attribute att;

		/*
		 * The attribute value of the antecedent. For numeric attribute, value is
		 * either 0(1st bag) or 1(2nd bag)
		 */
		protected double value;

		/*
		 * The attribute value of the antecedent for which the best info gain was
		 * achieved
		 */
		protected double bestValue;

		/*
		 * The maximum infoGain achieved by this antecedent test in the growing data
		 */
		protected double maxInfoGain;

		/* The accurate rate of this antecedent test on the growing data */
		protected double accuRate;

		/* The coverage of this antecedent in the growing data */
		protected double cover;

		/* The accurate data for this antecedent in the growing data */
		protected double accu;

		/* Constructor */
		public Antd(Attribute a) {
			att = a;
			value = Double.NaN;
			maxInfoGain = 0;
			accuRate = Double.NaN;
			cover = Double.NaN;
			accu = Double.NaN;
		}

		/* The abstract members for inheritance */
		public abstract Instances[] splitData(Instances data, double defAcRt,
				double cla);

		public abstract boolean covers(Instance inst);

		public abstract String toString();
		


		/** Implements Copyable */
		public abstract Object copy();

		/* Get functions of this antecedent */
		public Attribute getAttr() {
			return att;
		}

		public double getAttrValue() {
			return value;
		}

		public void setAttrValue(double v) {
			value = v;
		}

		public double getMaxInfoGain() {
			return maxInfoGain;
		}

		public double getAccuRate() {
			return accuRate;
		}

		public double getAccu() {
			return accu;
		}

		public double getCover() {
			return cover;
		}

		public double getBestAttrValue() {
			return bestValue;
		}

		public void setBestAttrValue(double currentValue) {
			this.bestValue = currentValue;
		}

		public void setAccu(double accu) {
			this.accu = accu;
		}

		public void setAccuRate(double accuRate) {
			this.accuRate = accuRate;
		}

		public void setCover(double cover) {
			this.cover = cover;
		}

		public void setMaxInfoGain(double maxInfoGain) {
			this.maxInfoGain = maxInfoGain;
		}
	}

	/**
	 * The antecedent with numeric attribute
	 */
	private class NumericAntd extends Antd {

		/* The split point for this numeric antecedent */
		private double splitPoint;

		/* Whether the positive class is on the lt or ge side */
		private boolean lt;

		/* Constructor */
		public NumericAntd(Attribute a) {
			super(a);
			splitPoint = Double.NaN;
			lt = false;
		}

		/* Get split point of this numeric antecedent */
		public double getSplitPoint() {
			return splitPoint;
		}
		
		/* Set the split point of this numeric antecedent */
		public void setSplitPoint(double sp) {
			splitPoint = sp;
		}

		/** Implements Copyable */
		public Object copy() {
			NumericAntd na = new NumericAntd(getAttr());
			na.value = this.value;
			na.splitPoint = this.splitPoint;
			return na;
		}

		/**
		 * Implements the splitData function. This procedure is to split the data
		 * into two bags according to the information gain of the numeric attribute
		 * value The maximum infoGain is also calculated.
		 * 
		 * @param insts
		 *          the data to be split
		 * @param defAcRt
		 *          the default accuracy rate for data
		 * @param cl
		 *          the class label to be predicted
		 * @return the array of data after split
		 */
		public Instances[] splitData(Instances insts, double defAcRt, double cl) {
			Instances data = insts;
			int total = data.numInstances();// Total number of instances without
			// missing value for att

			int split = 1; // Current split position
			int prev = 0; // Previous split position
			int finalSplit = split; // Final split position
			maxInfoGain = 0;
			value = 0;

			double fstCover = 0, sndCover = 0, fstAccu = 0, sndAccu = 0;

			data.sort(att);
			// Find the las instance without missing value
			for (int x = 0; x < data.numInstances(); x++) {
				Instance inst = data.instance(x);
				if (inst.isMissing(att)) {
					total = x;
					break;
				}

				sndCover += inst.weight();
				if (Utils.eq(inst.classValue(), cl))
					sndAccu += inst.weight();
			}

			if (total == 0)
				return null; // Data all missing for the attribute
			splitPoint = data.instance(total - 1).value(att);

			for (; split <= total; split++) {
				if ((split == total) || (data.instance(split).value(att) > // Can't
						// split
						// within
						data.instance(prev).value(att))) { // same value

					for (int y = prev; y < split; y++) {
						Instance inst = data.instance(y);
						fstCover += inst.weight();
						if (Utils.eq(data.instance(y).classValue(), cl)) {
							fstAccu += inst.weight(); // First bag positive# ++
						}
					}

					double fstAccuRate = (fstAccu + 1.0) / (fstCover + 1.0), sndAccuRate = (sndAccu + 1.0)
							/ (sndCover + 1.0);

					/* Which bag has higher information gain? */
					boolean isFirst;
					double fstInfoGain, sndInfoGain;
					double accRate, infoGain, coverage, accurate;

					fstInfoGain =
					// Utils.eq(defAcRt, 1.0) ?
					// fstAccu/(double)numConds :
					fstAccu * (Utils.log2(fstAccuRate) - Utils.log2(defAcRt));

					sndInfoGain =
					// Utils.eq(defAcRt, 1.0) ?
					// sndAccu/(double)numConds :
					sndAccu * (Utils.log2(sndAccuRate) - Utils.log2(defAcRt));

					if (fstInfoGain > sndInfoGain) {
						isFirst = true;
						infoGain = fstInfoGain;
						accRate = fstAccuRate;
						accurate = fstAccu;
						coverage = fstCover;
					} else {
						isFirst = false;
						infoGain = sndInfoGain;
						accRate = sndAccuRate;
						accurate = sndAccu;
						coverage = sndCover;
					}

					/* Check whether so far the max infoGain */
					if (infoGain > maxInfoGain) {
						splitPoint = data.instance(prev).value(att);
						value = (isFirst) ? 0 : 1;
						accuRate = accRate;
						accu = accurate;
						cover = coverage;
						maxInfoGain = infoGain;
						finalSplit = (isFirst) ? split : prev;
					}

					for (int y = prev; y < split; y++) {
						Instance inst = data.instance(y);
						sndCover -= inst.weight();
						if (Utils.eq(data.instance(y).classValue(), cl)) {
							sndAccu -= inst.weight(); // Second bag positive# --
						}
					}
					prev = split;
				}
			}

			/* Split the data */
			ModifiedInstances[] splitData = new ModifiedInstances[2];
			splitData[0] = new ModifiedInstances(data, 0, finalSplit);
			splitData[1] = new ModifiedInstances(data, finalSplit, total - finalSplit);

			return splitData;
		}

		/**
		 * Whether the instance is covered by this antecedent
		 * 
		 * @param inst
		 *          the instance in question
		 * @return the boolean value indicating whether the instance is covered by
		 *         this antecedent
		 */
		public boolean covers(Instance inst) {
			boolean isCover = true;
			if (!inst.isMissing(att)) {
				if ((int) value == 0) { // First bag
					if (inst.value(att) > splitPoint)
						isCover = false;
				} else if (inst.value(att) < splitPoint) // Second bag
					isCover = false;
			} else
				isCover = false;

			return isCover;
		}

		/**
		 * Prints this antecedent
		 * 
		 * @return a textual description of this antecedent
		 */
		public String toString() {
			String symbol = ((int) value == 0) ? " <= " : " >= ";
			return (att.name() + symbol + Utils.doubleToString(splitPoint, 6));
		}
		

	}

	/**
	 * The antecedent with nominal attribute
	 */
	private class NominalAntd extends Antd {

		/*
		 * The parameters of infoGain calculated for each attribute value in the
		 * growing data
		 */
		private double[] accurate;

		private double[] coverage;

		/* Constructor */
		public NominalAntd(Attribute a) {
			super(a);
			int bag = att.numValues();
			accurate = new double[bag];
			coverage = new double[bag];
		}

		/** Implements Copyable */
		public Object copy() {
			Antd antec = new NominalAntd(getAttr());
			antec.value = this.value;
			return antec;
		}

		/**
		 * Implements the splitData function. This procedure is to split the data
		 * into bags according to the nominal attribute value The infoGain for each
		 * bag is also calculated.
		 * 
		 * @param data
		 *          the data to be split
		 * @param defAcRt
		 *          the default accuracy rate for data
		 * @param cl
		 *          the class label to be predicted
		 * @return the array of data after split
		 */
		public Instances[] splitData(Instances data, double defAcRt, double cl) {
			int bag = att.numValues();
			Instances[] splitData = new ModifiedInstances[bag];

			for (int x = 0; x < bag; x++) {
				splitData[x] = new ModifiedInstances(data, data.numInstances());
				accurate[x] = 0;
				coverage[x] = 0;
			}

			for (int x = 0; x < data.numInstances(); x++) {
				Instance inst = data.instance(x);
				if (!inst.isMissing(att)) {
					int v = (int) inst.value(att);
					splitData[v].add(inst);
					coverage[v] += inst.weight();
					if ((int) inst.classValue() == (int) cl)
						accurate[v] += inst.weight();
				}
			}

			for (int x = 0; x < bag; x++) {
				double t = coverage[x] + 1.0;
				double p = accurate[x] + 1.0;
				double infoGain =
				// Utils.eq(defAcRt, 1.0) ?
				// accurate[x]/(double)numConds :
				accurate[x] * (Utils.log2(p / t) - Utils.log2(defAcRt));

				if (infoGain > maxInfoGain) {
					maxInfoGain = infoGain;
					cover = coverage[x];
					accu = accurate[x];
					accuRate = p / t;
					value = (double) x;
				}
			}

			return splitData;
		}

		/**
		 * Whether the instance is covered by this antecedent
		 * 
		 * @param inst
		 *          the instance in question
		 * @return the boolean value indicating whether the instance is covered by
		 *         this antecedent
		 */
		public boolean covers(Instance inst) {
			boolean isCover = false;
			if (!inst.isMissing(att)) {
				if ((int) inst.value(att) == (int) value)
					isCover = true;
			}
			return isCover;
		}

		/**
		 * Prints this antecedent
		 * 
		 * @return a textual description of this antecedent
		 */
		public String toString() {
			return (att.name() + " = " + att.value((int) value));
		}
	}

	/**
	 * This class implements a single rule that predicts specified class.
	 * 
	 * A rule consists of antecedents "AND"ed together and the consequent (class
	 * value) for the classification. In this class, the Information Gain
	 * (p*[log(p/t) - log(P/T)]) is used to select an antecedent and Reduced Error
	 * Prunning (REP) with the metric of accuracy rate p/(p+n) or (TP+TN)/(P+N) is
	 * used to prune the rule.
	 */

	protected class RipperRule extends Rule {

		/** The internal representation of the class label to be predicted */
		private double m_Consequent = -1;

		/** The vector of antecedents of this rule */
		protected FastVector m_Antds = null;

		public void setConsequent(double cl) {
			m_Consequent = cl;
		}

		public double getConsequent() {
			return m_Consequent;
		}

		/** Constructor */
		public RipperRule() {
			m_Antds = new FastVector();
		}

		/**
		 * Get a shallow copy of this rule
		 * 
		 * @return the copy
		 */
		public Object copy() {
			RipperRule copy = new RipperRule();
			copy.setConsequent(getConsequent());
			copy.m_Antds = (FastVector) this.m_Antds.copyElements();
			return copy;
		}

		/**
		 * Whether the instance covered by this rule
		 * 
		 * @param inst
		 *          the instance in question
		 * @return the boolean value indicating whether the instance is covered by
		 *         this rule
		 */
		public boolean covers(Instance datum) {
			boolean isCover = true;

			for (int i = 0; i < m_Antds.size(); i++) {
				Antd antd = (Antd) m_Antds.elementAt(i);
				if (!antd.covers(datum)) {
					isCover = false;
					break;
				}
			}

			return isCover;
		}

		/**
		 * Whether this rule has antecedents, i.e. whether it is a default rule
		 * 
		 * @return the boolean value indicating whether the rule has antecedents
		 */
		public boolean hasAntds() {
			if (m_Antds == null)
				return false;
			else
				return (m_Antds.size() > 0);
		}

		/**
		 * the number of antecedents of the rule
		 * 
		 * @return the size of this rule
		 */
		public double size() {
			return (double) m_Antds.size();
		}

		/**
		 * Private function to compute default number of accurate instances in the
		 * specified data for the consequent of the rule
		 * 
		 * @param data
		 *          the data in question
		 * @return the default accuracy number
		 */
		private double computeDefAccu(Instances data) {
			double defAccu = 0;
			Attribute covered = m_Covered;
			int positive = covered.indexOfValue("+");
			for (int i = 0; i < data.numInstances(); i++) {
				Instance inst = data.instance(i);
				float cov = inst.value(covered);
	    	boolean notCovered = Double.isNaN(cov) || (int)cov != positive;
				if (notCovered&&(int) inst.classValue() == (int) m_Consequent)
					defAccu += inst.weight();
			}
			return defAccu;
		}

		/**
		 * Function to apply the rule and compute the pred_class attribute.
		 * 
		 * @param data
		 *          the data in question
		 * @param useCovered
		 *          if true, when the rule does not cover the instance, the instance
		 *          recieves the value of the "covered" attribute if false, the
		 *          instance recieves "-" if not covered by the rule
		 * @return data with the pred_class attribute modified
		 */
		public Instances apply(Instances data, boolean useCovered) {
			Attribute predClass = m_PredictedClass;
			int positive = predClass.indexOfValue("+");
			// System.err.println("Applying to attr " + predClass);
			Attribute covered = m_Covered;
			int numCovered = 0, numNewCovered = 0, totalCovered=0;
			for (int i = 0; i < data.numInstances(); i++) {
				Instance inst = data.instance(i);
				float cov = inst.value(covered);
				if (covers(inst)){
					inst.setValue(predClass, "+");
					if (Float.isNaN(cov)||(int)cov!=positive)
						numNewCovered+=inst.weight();
					numCovered+=inst.weight();
					totalCovered++;
				}else {
					if (useCovered) {						
						if (Float.isNaN(cov))
							inst.setValue(predClass, "-");
						else{
							inst.setValue(predClass, cov);
							if((int)cov==positive){
								numCovered+=inst.weight();
								totalCovered++;
							}
						}
					} else
						inst.setValue(predClass, "-");
				}
			}
			if(m_Debug)
				System.err.println("After apply: "+numCovered+" ("+totalCovered+") total; "
						+numNewCovered+" new.");
			return data;
		}

		/**
		 * Function to apply the rule and compute the pred_class attribute. This
		 * variation only apply the first num antecedents from the rule.
		 * 
		 * @param data
		 *          the data in question
		 * @param num
		 *          the index of the last antecedent to be applied
		 * @return data with the pred_class attribute modified
		 */
		public Instances apply(Instances data, int num) {
			Attribute predClass = m_PredictedClass;
			Attribute covered = m_Covered;
			int end = num < m_Antds.size() ? num + 1 : m_Antds.size();
			for (int i = 0; i < data.numInstances(); i++) {
				Instance inst = data.instance(i);
				double cov = inst.value(covered);
				if (Double.isNaN(cov) || (int) cov != covered.indexOfValue("+")) {
					// Not covered, so compute the predicted class
					boolean isCovered = true;
					for (int j = 0; j < end; j++) {
						Antd antd = (Antd) m_Antds.elementAt(j);
						if (!antd.covers(inst)) {
							isCovered = false;
						}
					}
					if (isCovered)
						inst.setValue(predClass, "+");
					else
						inst.setValue(predClass, "-");
				}else //Covered, mark as +
					inst.setValue(predClass, "+");
			}
			return data;
		}

		/**
		 * Private function to apply the rule and compute the pred_class attribute.
		 * This variation applies the antecedents between begin and end. This
		 * variation assumes incremental application
		 * 
		 * @param data
		 *          the data in question
		 * @param begin
		 *          the index of the first antecedent to be applied
		 * @param end
		 *          the index of the last antecedent to be applied
		 * @return data with the pred_class attribute modified
		 */
		private Instances apply(Instances data, int begin, int end) {
			if (begin <= 0)
				return apply(data, end);
			Attribute predClass = m_PredictedClass;
			Attribute covered = m_Covered;
			int endInd = end < m_Antds.size() ? end + 1 : m_Antds.size();
			for (int i = 0; i < data.numInstances(); i++) {
				Instance inst = data.instance(i);
				double cov = inst.value(covered);
				if ((Double.isNaN(cov) || (int) cov != covered.indexOfValue("+"))
						&& (int) inst.value(predClass) == predClass.indexOfValue("+")) {
					// Not covered, so compute the predicted class
					boolean isCovered = true;
					for (int j = begin; j < endInd; j++) {
						Antd antd = (Antd) m_Antds.elementAt(j);
						if (!antd.covers(inst)) {
							isCovered = false;
						}
					}
					if (isCovered)
						inst.setValue(predClass, "+");
					else
						inst.setValue(predClass, "-");
				}
			}
			return data;
		}

		/**
		 * Private function to apply all rules in a ruleset and compute the
		 * covered attribute.
		 * 
		 * @param ruleset The ruleset.
		 * @param data
		 *          the data in question
		 * @param endPosition The position of the first rule that is not to be applied.
		 * @return data with the covered attribute modified
		 */
		public Instances applyWhileSkipping(FastVector ruleset, Instances data, int index, int endPosition) {
			Attribute cover = m_Covered;
			int end = endPosition < ruleset.size()? endPosition: ruleset.size();
			for (int j = 0; j < data.numInstances(); j++) {
				Instance inst = data.instance(j);
				inst.setValue(cover, "-");
				for (int i = 0; i < end; i++) {
					if (i != index) {
						RipperRule rule = (RipperRule) ruleset.elementAt(i);
						if (rule.covers(inst))
							inst.setValue(cover, "+");
					}
				}
			}
			data = transitiveClosure(data, cover);
			return data;
		}
		
		/**
		 * Private function to apply all rules in a ruleset and compute the
		 * covered attribute.
		 * 
		 * @param ruleset The ruleset.
		 * @param data
		 *          the data in question
		 * @param endPosition The position of the first rule that is not to be applied.
		 * @return data with the covered attribute modified
		 */
		public Instances apply(FastVector ruleset, Instances data, int endPosition) {
			Attribute cover = m_Covered;
			int end = endPosition < ruleset.size()? endPosition: ruleset.size();
			for (int j = 0; j < data.numInstances(); j++) {
				Instance inst = data.instance(j);
				inst.setValue(cover, "-");
				for (int i = 0; i < end; i++) {
					RipperRule rule = (RipperRule) ruleset.elementAt(i);
					if (rule.covers(inst))
						inst.setValue(cover, "+");
				}
			}
			data = transitiveClosure(data, cover);
			return data;
		}
		
		/**
		 * Private function to apply all rules in a ruleset and compute the
		 * covered attribute.
		 * 
		 * @param ruleset The ruleset.
		 * @param data
		 *          the data in question
		 * @return data with the pred_class attribute modified
		 */
		private Instances apply(FastVector ruleset, Instances data){
			return apply(ruleset, data, ruleset.size());
		}
		/**
		 * Private function to apply all rules in a ruleset excluding rule ruleNum
		 * and compute the covered attribute.
		 * 
		 * @param ruleset The ruleset.
		 * @param data
		 *          the data in question
		 * @param ruleNum The index of the rule that is to be skipped.
		 * @return data with the pred_class attribute modified
		 */
		public Instances applyWhileSkipping(FastVector ruleset, Instances data, int ruleNum) {
			Attribute cover = m_Covered;
			for (int j = 0; j < data.numInstances(); j++) {
				Instance inst = data.instance(j);
				inst.setValue(cover, "-");
				for (int i = 0; i < ruleset.size(); i++) {
					if (i != ruleNum) {
						RipperRule rule = (RipperRule) ruleset.elementAt(i);
						if (rule.covers(inst))
							inst.setValue(cover, "+");
					}
				}
			}
			//data = transitiveClosure(data, cover);
			return data;
		}

		public Instances clear(Instances data, boolean useCovered) {
			Attribute predClass = m_PredictedClass;
			Attribute cover = m_Covered;
			for (int i = 0; i < data.numInstances(); i++) {
				Instance inst = data.instance(i);
				if(useCovered)
					inst.setValue(predClass, inst.value(cover));
				else
					inst.setValue(predClass, "-");
			}
			return data;
		}

		/**
		 * Private function to check the effect of adding an antecedent to the rule
		 * 
		 * @param data
		 *          the data in question
		 * @param antecedent
		 *          the new antecedent under consideration.
		 * @return true if the antecedent covers at least one positive instance
		 */
		private boolean applyWithAntd(Instances data, Antd ant) {
			if (m_Debug && ves_Debug)
				System.err.println("Ant " + ant);
			int covers = 0;
			Attribute realClass = m_RealClass;
			Attribute covered = m_Covered;
			int pos_index = covered.indexOfValue("+");
			for (int i = 0; i < data.numInstances(); i++) {
				Instance inst = data.instance(i);
				double val = inst.value(covered);
				if (Double.isNaN(val) || (int) val != pos_index) {
					if (!ant.covers(inst))
						inst.setValue(realClass, "-");
					else {
						if (covers(inst)) {
							inst.setValue(realClass, pos_index);
							covers += inst.weight();
						} else {
							inst.setValue(realClass, "-");
						}
					}
				}else{
					inst.setValue(realClass, pos_index);
				}
			}
			//if(m_Debug)
			//	System.err.print("| "+covers);
			return covers>0;
		}

		/**
		 * Build one rule using the growing data
		 * 
		 * @param growData
		 *          the growing data used to build the rule
		 * @exception if
		 *              the consequent is not set yet
		 */
		public void grow(Instances growData) throws Exception {
			if (m_Consequent == -1)
				throw new Exception(" Consequent not set yet.");

			/* Compute the default accurate rate of the growing data */
			growData = apply(growData, true);
			growData = transitiveClosure(growData, m_PredictedClass);
			growData = getCovered(growData);
			double sumOfWeights = growData.sumOfWeights();
			if (!Utils.gr(sumOfWeights, 0.0))
				return;

			double defAccu = computeDefAccu(growData);
			double defAcRt = (defAccu + 1.0) / (sumOfWeights + 1.0);
			//System.err.println("Documents: "+((ModifiedInstances)growData).printDocOrder());
			if (m_Debug && ves_Debug) {
				System.err.println("^^^^^^^^^^\n" + growData.toString(false));
				System.err.println("Growing rule " + this.toString(m_Class)
						+ " default accuracy = " + defAccu);
				// System.err.println(data.toString(false));
			}

			/* Keep the record of which attributes have already been used */
			boolean[] used = new boolean[growData.numAttributes()];
			for (int k = 0; k < used.length; k++)
				used[k] = false;
			int numUnused = used.length;

			// If there are already antecedents existing
			for (int j = 0; j < m_Antds.size(); j++) {
				Antd antdj = (Antd) m_Antds.elementAt(j);
				if (!antdj.getAttr().isNumeric()) {
					used[antdj.getAttr().index()] = true;
					numUnused--;
				}
			}

			double maxInfoGain;
			//System.err.println(growData.numInstances());
			while (Utils.gr(growData.numInstances(), 0.0) && (numUnused > 0)
					&& Utils.sm(defAcRt, 1.0)) {

				// We require that infoGain be positive
				/*
				 * if(numAntds == originalSize) maxInfoGain = 0.0; // At least one
				 * condition allowed else maxInfoGain = Utils.eq(defAcRt, 1.0) ?
				 * defAccu/(double)numAntds : 0.0;
				 */
				maxInfoGain = 0.0;

				/* Build a list of antecedents */
				Antd oneAntd = null;
				//Instances coverData = null;
				Enumeration enumAttr = growData.enumerateAttributes();
				
				if (m_Debug){
					NumberFormat nf = NumberFormat.getInstance();
					nf.setMaximumFractionDigits(2);
					System.err.println("\nOne condition: size = " + nf.format(growData.sumOfWeights())
							+". Total instances " + nf.format(growData.numInstances())+ ". Default accuracy "+nf.format(defAcRt));
				}
				/* Build one condition based on all attributes not used yet */
				while (enumAttr.hasMoreElements()) {
					Attribute att = (Attribute) (enumAttr.nextElement());

					Antd antd = null;
					if (att.isNumeric())
						antd = new NumericAntd(att);
					else
						antd = new NominalAntd(att);

					if (!used[att.index()]) {
						/*
						 * Compute the best information gain for each attribute, it's stored
						 * in the antecedent formed by this attribute. This procedure
						 * returns the data covered by the antecedent
						 */
						// Instances coveredData = computeInfoGain(growData, defAcRt, antd);
						double infoGain = computeInfoGain(growData, defAcRt, antd);
						double cover = antd.getCover();
						if (Utils.gr(cover, 0)) {
							// double infoGain = antd.getMaxInfoGain();

							if (infoGain > maxInfoGain) {
								if (m_Debug){
									NumberFormat nf = NumberFormat.getInstance();
									nf.setMaximumFractionDigits(2);
									System.err.println( "\tInfoGain = " + nf.format(infoGain) + " | Accuracy = "+ 
											nf.format(antd.getAccuRate()) + "=" + nf.format(antd.getAccu()) 
											+ "/"	+ nf.format(antd.getCover()) + " for attribute " + 
											antd.toString() +".");
								}
								//if (m_Debug)
								//	System.err.println(" ***New best****");
								oneAntd = antd;
								maxInfoGain = infoGain;
							}							
						}
					}
				}

				if (oneAntd == null){
					if(m_Debug)
						System.err.println("Antecedent not found. Quiting.");
					break; // Cannot find antds
				}
				if (Utils.sm(oneAntd.getAccu(), m_MinNo)){
					if(m_Debug)
						System.err.println("Coverage lower than "+m_MinNo+". Quiting.");
					break;// Too low coverage
				}
				// Numeric attributes can be used more than once
				if (!oneAntd.getAttr().isNumeric()) {
					used[oneAntd.getAttr().index()] = true;
					numUnused--;
				}

				m_Antds.addElement(oneAntd);
				growData = getCoveredData(growData, oneAntd);// Grow data size is shrinking
				defAcRt = oneAntd.getAccuRate();
				if (m_Debug) {
					System.err.println("\nAdding anticedent " + oneAntd + " accuracy "
							+ defAcRt);
				}
				//growData = apply(growData, true);
				//growData = transitiveClosure(growData, m_PredictedClass);
				//boolean outputHeader = false;
				//for(int i = 0; i<growData.numInstances(); i++){
				//	Instance ins = growData.instance(i);
				//	if(ins.value(m_RealClass)!=ins.value(m_PredictedClass)){
				//		if(!outputHeader){
				//			System.err.print(growData.printHeader());
				//			outputHeader = true;
				//		}
				//		System.err.println(ins);
				//	}
				//}
			}
			if (m_Debug && ves_Debug)
				System.err.println("DefAcRate " + defAcRt + " instances "
						+ growData.numInstances());
		}

		/**
		 * Compute the best information gain for the specified antecedent
		 * 
		 * @param instances
		 *          the data based on which the infoGain is computed
		 * @param defAcRt
		 *          the default accuracy rate of data
		 * @param antd
		 *          the specific antecedent
		 * @param numConds
		 *          the number of antecedents in the rule so far
		 * @return the data covered by the antecedent
		 */
		// private Instances computeInfoGain(Instances instances, double defAcRt,
		// Antd antd) {
		// Instances data = instances;
		//
		// /*
		// * Split the data into bags. The information gain of each bag is also
		// * calculated in this procedure
		// */
		// Instances[] splitData = antd.splitData(data, defAcRt, m_Consequent);
		//
		// /* Get the bag of data to be used for next antecedents */
		// if(splitData != null)
		// return splitData[(int)antd.getAttrValue()];
		// else
		// return null;
		// }
		//    
		/**
		 * Compute the best information gain for the specified antecedent
		 * 
		 * @param instances
		 *          the data based on which the infoGain is computed
		 * @param defAcRt
		 *          the default accuracy rate of data
		 * @param antd
		 *          the specific antecedent
		 * @param numConds
		 *          the number of antecedents in the rule so far
		 * @return the info gain for the antecedent
		 */
		private double computeInfoGain(Instances instances, double defAcRt,
				Antd antd) {
			Instances data = instances;// , result[];
			double maxInfoGain = 0;
			Attribute currentClass = m_RealClass;
			Attribute covered = m_Covered;
			int indexOfPositive = currentClass.indexOfValue("+");
			Attribute trueClass = m_Class;
			int bag;
			if (antd.getAttr().isNominal()) {//Nominal antecedent
				bag = antd.getAttr().numValues();
				double coverage[] = new double[bag];
				double accurate[] = new double[bag];
				// result = new ModifiedInstances[bag];

				for (int v = 0; v < bag; v++) {
					// result[v] = new ModifiedInstances(data, data.numInstances());
					// Try all possible values for the antecedent
					antd.setAttrValue(v);
					boolean covers = applyWithAntd(data, antd);
					if (!covers) { // this antecedent doesn't cover any instances; skip
						coverage[v] = 0;
						continue;
					}
					data = transitiveClosure(data, currentClass);
					if (m_Debug && ves_Debug)
						System.err.println("Trying " + antd + ". After transitive closure:"
								+ data.toString(false));
					for (int i = 0; i < data.numInstances(); i++) {
						Instance current = data.instance(i);
						// if this instance is not covered by a previous rule
						double cov = current.value(covered);

						int predicted = (int) current.value(currentClass);
						int trueCl = (int) current.value(trueClass);
						boolean positive = predicted == indexOfPositive;
						boolean correct = predicted == trueCl;
						if (positive) {
							if (Double.isNaN(cov) || (int) cov != indexOfPositive) {
								coverage[v] += current.weight();
								if (correct)
									accurate[v] += current.weight();
							}
						}
					}
					//if(m_Debug){
					//	System.err.print((int)coverage[v]+"("+(int)accurate[v]+") ");
					//}
				}

				double cover = 0, accu = Double.NaN, accuRate = Double.NaN, val = 0;
				int ind = -1;
				for (int x = 0; x < bag; x++) {
					double t = coverage[x] + 1.0;
					double p = accurate[x] + 1.0;
					double infoGain = accurate[x]
							* (Utils.log2(p / t) - Utils.log2(defAcRt));
					// if (m_Debug)
					// System.err.println(antd+" => " + infoGain + "=" + accurate[x]
					// + "(*" + Utils.log2(p / t) + "-" + Utils.log2(defAcRt) + ")");
					if (infoGain > maxInfoGain) {
						maxInfoGain = infoGain;
						cover = coverage[x];
						accu = accurate[x];
						accuRate = p / t;
						val = (double) x;
						ind = x;
					}
				}
				// Store all the necessary data with the antecedent.
				if (m_Debug && ves_Debug)
					System.err.println("Cover " + cover + "; Accurate " + accu
							+ ";InfoGain " + maxInfoGain);
				antd.maxInfoGain = maxInfoGain;
				antd.cover = cover;
				antd.accu = accu;
				antd.accuRate = accuRate;
				antd.value = val;
				antd.bestValue = val;
				if (ind < 0)
					return 0;
				return maxInfoGain;
			} else { //Numeric antecedent
				bag = antd.getAttr().countAndComputeValues(instances);
				double offset = 1;
				if(m_Debug)
					System.err.print("\t*Numeric antd "+antd+" with "+bag+" values. ");
				if(bag>m_MaxNumAttributes){
					//m_MaxNumAttributes = bag;
					offset = ((double)bag)/((double)m_MaxNumAttributes);
					bag = m_MaxNumAttributes;
					if(m_Debug)
						System.err.println("Reducing to "+m_MaxNumAttributes+" values.");
				}else{
					if(m_Debug)
						System.err.println();
				}
					
				double coverage[] = new double[2*bag];
				double accurate[] = new double[2*bag];
				// result = new ModifiedInstances[bag];
				for (int b = 0; b < bag; b++) {
					// result[v] = new ModifiedInstances(data, data.numInstances());
					// Try all possible values for the antecedent
					int value = (int)(b*offset);
					for (int le = 0; le <= 1; le++) {
						NumericAntd nantd = (NumericAntd) antd;
						nantd.setAttrValue(le);
						nantd.setSplitPoint(nantd.getAttr().getValNum(value));
						boolean covers = applyWithAntd(data, antd);
						if (!covers) { // this antecedent doesn't cover any instances; skip
							coverage[2 * b + le] = 0;
							continue;
						}
						data = transitiveClosure(data, currentClass);
						if (m_Debug && ves_Debug)
							System.err.println("Trying " + antd
									+ ". After transitive closure:" + data.toString(false));
						for (int i = 0; i < data.numInstances(); i++) {
							Instance current = data.instance(i);
							// if this instance is not covered by a previous rule
							double cov = current.value(covered);

							int predicted = (int) current.value(currentClass);
							int trueCl = (int) current.value(trueClass);
							boolean positive = predicted == indexOfPositive;
							boolean correct = predicted == trueCl;
							if (positive) {
								if (Double.isNaN(cov) || (int) cov != indexOfPositive) {
									coverage[2*b+le] += current.weight();
									if (correct)
										accurate[2*b+le] += current.weight();
								}
							}
						}
					}
				}

				double cover = 0, accu = Double.NaN, accuRate = Double.NaN, val = 0;
				int ind = -1;
				for (int b = 0; b < bag; b++) {
					for (int le=0; le < 1; le++) {
						double t = coverage[2 * b + le] + 1.0;
						double p = accurate[2 * b + le] + 1.0;
						double infoGain = accurate[2 * b + le]
								* (Utils.log2(p / t) - Utils.log2(defAcRt));
						// if (m_Debug)
						// System.err.println(antd+" => " + infoGain + "=" + accurate[x]
						// + "(*" + Utils.log2(p / t) + "-" + Utils.log2(defAcRt) + ")");
						if (infoGain > maxInfoGain) {
							maxInfoGain = infoGain;
							cover = coverage[2 * b + le];
							accu = accurate[2 * b + le];
							accuRate = p / t;
							val = le;
							((NumericAntd) antd).setSplitPoint(antd.getAttr().getValNum((int)(b*offset)));
							ind = (int)(b*offset);
						}
					}
				}
				// Store all the necessary data with the antecedent.
				if (m_Debug && ves_Debug)
					System.err.println("Cover " + cover + "; Accurate " + accu
							+ ";InfoGain " + maxInfoGain);
				antd.maxInfoGain = maxInfoGain;
				antd.cover = cover;
				antd.accu = accu;
				antd.accuRate = accuRate;
				antd.value = val;
				antd.bestValue = val;
				if (ind < 0)
					return 0;
				return maxInfoGain;
			}
		}
		

		private Instances getCoveredData(Instances data, Antd antd){
			Attribute realClass = m_RealClass;
			Attribute covered = m_Covered;
			int indexOfPositive = realClass.indexOfValue("+");
			Instances result = new ModifiedInstances(data,data.numInstances());
			
			applyWithAntd(data, antd);
			data = transitiveClosure(data, realClass);

			for (int i = 0; i < data.numInstances(); i++) {
				Instance current = data.instance(i);
				// if this instance is not covered by a previous rule
				double val = current.value(covered);

				int predicted = (int) current.value(realClass);
				boolean positive = predicted == indexOfPositive;
				if (positive) {
					result.add(current);
				}else
					if (!Double.isNaN(val) && (int) val == indexOfPositive)
						result.add(current);
			}
			return result;
		}
		
		private Instances getCovered(Instances data) {
			Attribute predicted = m_PredictedClass;
			int indexOfPositive = predicted.indexOfValue("+");
			Instances result = new ModifiedInstances(data, data.numInstances());
			for (int i = 0; i < data.numInstances(); i++) {
				Instance current = data.instance(i);
				// if this instance is not covered by a previous rule

				int predictedValue = (int) current.value(predicted);
				boolean positive = predictedValue == indexOfPositive;
				if (positive)
					result.add(current);
			}
			return result;
		}
	

		

		/**
		 * Prune all the possible final sequences of the rule using the pruning
		 * data. The measure used to prune the rule is based on flag given.
		 * 
		 * @param pruneData
		 *          the pruning data used to prune the rule
		 * @param useWhole
		 *          flag to indicate whether use the error rate of the whole pruning
		 *          data instead of the data covered
		 */
		public void prune(Instances pruneData, boolean useWhole) {
			Instances data = pruneData;
			Attribute predictedClass = m_PredictedClass;
			if (m_Debug && ves_Debug)
				System.err.println("Pruning " + data.toString(false));

			int positive = predictedClass.indexOfValue("+");
			double total = data.sumOfUncovWeights(m_Covered);
			if (!Utils.gr(total, 0.0))
				return;

			/* The default accurate # and rate on pruning data */
			double defAccu = computeDefAccu(data);

			if (m_Debug)
				System.err.println("Pruning with " + defAccu + " positive data out of "
						+ total + " instances");

			int size = m_Antds.size();
			if (size == 0)
				return; // Default rule before pruning

			double[] worthRt = new double[size];
			double[] coverage = new double[size];
			double[] worthValue = new double[size];
			double[] tn = new double[size];
			for (int w = 0; w < size; w++) {
				tn[w] = worthRt[w] = coverage[w] = worthValue[w] = 0.0;
			}

			/* Calculate accuracy parameters for all the antecedents in this rule */
			for (int x = 0; x < size; x++) {
				//this.apply(data, x - 1, x);
				this.apply(data,x);
				transitiveClosure(data, predictedClass);
				if (m_Debug && ves_Debug)
					System.err.println(data.toString(false));
				// Antd antd = (Antd) m_Antds.elementAt(x);
				// Attribute attr = antd.getAttr();
				// Instances newData = data;
				// data = new ModifiedInstances(newData, 0); // Make data empty

				for (int y = 0; y < data.numInstances(); y++) {
					Instance ins = data.instance(y);
					
					double previouslyCovered = ins.value(m_Covered);
					boolean notPrevCovered = Double.isNaN(previouslyCovered) || (int)previouslyCovered != positive;
					boolean covered = (int) ins.value(predictedClass) == positive;
					
					if (notPrevCovered) {
						if(covered){
						// System.err.println("COVERED "+ins);
						coverage[x] += ins.weight();
						if ((int) ins.classValue() == positive) // Accurate prediction
							worthValue[x] += ins.weight();
						}else if (useWhole) { // Not covered
							if ((int) ins.classValue() != positive)
								tn[x] += ins.weight();
						}
					}
				}
				//System.err.println("Coverage: "+coverage[x]+". Worth: "+worthValue[x]+". tn "+tn);
				// for (int y = 0; y < newData.numInstances(); y++) {
				// Instance ins = newData.instance(y);
				//
				// if (antd.covers(ins)) { // Covered by this antecedent
				// coverage[x] += ins.weight();
				// data.add(ins); // Add to data for further pruning
				// if ((int) ins.classValue() == (int) m_Consequent) // Accurate
				// // prediction
				// worthValue[x] += ins.weight();
				// } else if (useWhole) { // Not covered
				// if ((int) ins.classValue() != (int) m_Consequent)
				// tn += ins.weight();
				// }
				// }

				if (useWhole) {
					worthValue[x] += tn[x];
					worthRt[x] = worthValue[x] / total;
				} else
					// Note if coverage is 0, accuracy is 0.5
					worthRt[x] = (worthValue[x] + 1.0) / (coverage[x] + 2.0);
			}

			double maxValue = (defAccu + 1.0) / (total + 2.0);
			int maxIndex = -1;
			for (int i = 0; i < worthValue.length; i++) {
				if (m_Debug) {
					double denom = useWhole ? total : coverage[i];
					System.err.println(i + "(useAccuray? " + !useWhole + "): "
							+ worthRt[i] + "=" + worthValue[i] + "/" + denom);
				}
				if (worthRt[i] > maxValue) { // Prefer to the
					maxValue = worthRt[i]; // shorter rule
					maxIndex = i;
				}
			}

			/* Prune the antecedents according to the accuracy parameters */
			for (int z = size - 1; z > maxIndex; z--)
				m_Antds.removeElementAt(z);
		}
		
		/**
		 * Prune all the possible final sequences of the rule using the pruning
		 * data. The measure used to prune the rule is based on flag given.
		 * This particular function measures the performance in the context of the
		 * entire ruleset, not just the particular rule.
		 * 
		 * Currently the same as prune();
		 * 
		 * @param pruneData
		 *          the pruning data used to prune the rule
		 * @param useWhole
		 *          flag to indicate whether use the error rate of the whole pruning
		 *          data instead of the data covered
		 */
		public void pruneWithRuleset(Instances pruneData, boolean useWhole) {
			prune(pruneData,useWhole);
		}
		/*
			Instances data = pruneData;
			Attribute predictedClass = m_PredictedClass;
			if (m_Debug && ves_Debug)
				System.err.println("Pruning " + data.toString(false));

			int positive = predictedClass.indexOfValue("+");
			double total = data.sumOfWeights();
			if (!Utils.gr(total, 0.0))
				return;

			// The default accurate # and rate on pruning data 
			double defAccu = computeDefAccu(data);

			if (m_Debug)
				System.err.println("Pruning with " + defAccu + " positive data out of "
						+ total + " instances");

			int size = m_Antds.size();
			if (size == 0)
				return; // Default rule before pruning

			double[] worthRt = new double[size];
			double[] coverage = new double[size];
			double[] worthValue = new double[size];
			for (int w = 0; w < size; w++) {
				worthRt[w] = coverage[w] = worthValue[w] = 0.0;
			}

			// Calculate accuracy parameters for all the antecedents in this rule 
			double tn = 0.0; // True negative if useWhole
			for (int x = 0; x < size; x++) {
				this.apply(data, x - 1, x);
				transitiveClosure(data, predictedClass);
				if (m_Debug && ves_Debug)
					System.err.println(data.toString(false));
				// Antd antd = (Antd) m_Antds.elementAt(x);
				// Attribute attr = antd.getAttr();
				// Instances newData = data;
				// data = new ModifiedInstances(newData, 0); // Make data empty

				for (int y = 0; y < data.numInstances(); y++) {
					Instance ins = data.instance(y);

					boolean covered = (int) ins.value(predictedClass) == positive;
					if (covered) {
						coverage[x] += ins.weight();
						if ((int) ins.classValue() == positive) // Accurate prediction
							worthValue[x] += ins.weight();
					} else if (useWhole) { // Not covered
						if ((int) ins.classValue() != positive)
							tn += ins.weight();
					}
				}
				// for (int y = 0; y < newData.numInstances(); y++) {
				// Instance ins = newData.instance(y);
				//
				// if (antd.covers(ins)) { // Covered by this antecedent
				// coverage[x] += ins.weight();
				// data.add(ins); // Add to data for further pruning
				// if ((int) ins.classValue() == (int) m_Consequent) // Accurate
				// // prediction
				// worthValue[x] += ins.weight();
				// } else if (useWhole) { // Not covered
				// if ((int) ins.classValue() != (int) m_Consequent)
				// tn += ins.weight();
				// }
				// }

				if (useWhole) {
					worthValue[x] += tn;
					worthRt[x] = worthValue[x] / total;
				} else
					// Note if coverage is 0, accuracy is 0.5
					worthRt[x] = (worthValue[x] + 1.0) / (coverage[x] + 2.0);
			}

			double maxValue = (defAccu + 1.0) / (total + 2.0);
			int maxIndex = -1;
			for (int i = 0; i < worthValue.length; i++) {
				if (m_Debug) {
					double denom = useWhole ? total : coverage[i];
					System.err.println(i + "(useAccuray? " + !useWhole + "): "
							+ worthRt[i] + "=" + worthValue[i] + "/" + denom);
				}
				if (worthRt[i] > maxValue) { // Prefer to the
					maxValue = worthRt[i]; // shorter rule
					maxIndex = i;
				}
			}

			// Prune the antecedents according to the accuracy parameters 
			for (int z = size - 1; z > maxIndex; z--)
				m_Antds.removeElementAt(z);
		}
	*/

		/**
		 * Prints this rule
		 * 
		 * @param classAttr
		 *          the class attribute in the data
		 * @return a textual description of this rule
		 */
		public String toString(Attribute classAttr) {
			if (classAttr == null)
				classAttr = m_Class;
			StringBuffer text = new StringBuffer();
			if (m_Antds.size() > 0) {
				for (int j = 0; j < (m_Antds.size() - 1); j++)
					text.append("(" + ((Antd) (m_Antds.elementAt(j))).toString()
							+ ") and ");
				text.append("(" + ((Antd) (m_Antds.lastElement())).toString() + ")");
			}
			text.append(" => " + classAttr.name() + "="
					+ classAttr.value((int) m_Consequent));

			return text.toString();
		}
		
		/*
		 * Print the rule
		 */
		
		public String toString(){
			return toString(m_Class);
		}
	/**
	 * Prints this rule in the RIPPER traditional representation
	 * 
	 * @return a textual description of this rule
	 */
	public String toRIPPERString() {
			StringBuffer text = new StringBuffer();
			if (m_Antds.size() > 0) {
				for (int j = 0; j < m_Antds.size(); j++)
					text.append(((Antd) (m_Antds.elementAt(j))).toString() + " ");
			}
			text.append(".");
			return text.toString();
		}
	}
	/**
	 * Builds Ripper in the order of class frequencies. For each class it's built
	 * in two stages: building and optimization
	 * 
	 * @param instances
	 *          the training data
	 * @exception Exception
	 *              if classifier can't be built successfully
	 */
	public void buildClassifier(Instances instances) throws Exception {
	    System.err.println("StRip with options: "+Utils.joinOptions(getOptions())+
	        " docs="+instances.getNumDocuments()+" features="+instances.numAttributes());
		// System.err.println("Called builClassifier:"+instances);
		if (instances.numInstances() == 0)
			throw new Exception(" No instances with a class value!");

		if (instances.checkForStringAttributes())
			throw new UnsupportedAttributeTypeException(" Cannot handle string attributes!");

		if (!instances.classAttribute().isNominal())
			throw new UnsupportedClassTypeException(" Only nominal class, please.");

		m_Class = instances.classAttribute();
		// A hack to use the modified class for instances
		if (m_Instances == null) {
			//instances = new ModifiedInstances(instances);
			// Add a new attribute to the instances, needed in the algorithm
		  //System.err.println("*** L = "+m_Ratio +" ****");
		    if(m_Ratio!=1){
		      ((ModifiedInstances)instances).setInstWeights(m_Class, m_Ratio);
		      System.err.println("Setting ratio to "+m_Ratio);
		    }
			Attribute realClass = m_Class.copy("real_class");
			Attribute predClass = realClass.copy("predicted_class");
			Attribute covered = realClass.copy("covered");
			instances.insertAttributeAt(realClass, instances.numAttributes());
			instances.insertAttributeAt(predClass, instances.numAttributes());
			instances.insertAttributeAt(covered, instances.numAttributes());
			m_RealClass = instances.attribute("real_class");
			m_PredictedClass = instances.attribute("predicted_class");
			m_Covered = instances.attribute("covered");
			m_Instances = instances;
		}else // only run once
			return;

		m_Random = instances.getRandomNumberGenerator(m_Seed);
		instances = m_Instances;

		m_Total = StRuleStats.numAllConditions(instances);
		if (m_Debug)
			System.err.println("Number of all possible conditions = " + m_Total);

		/*
		 * Removed. I don't want to change the order of the examples!!! m_Filter =
		 * new ClassOrder(); ((ClassOrder)m_Filter).setSeed(m_Random.nextInt());
		 * ((ClassOrder)m_Filter).setClassOrder(ClassOrder.FREQ_ASCEND);
		 * m_Filter.setInputFormat(instances); Instances modData = new
		 * ModifiedInstances(Filter.useFilter(instances, m_Filter));
		 */
		if (m_Debug && ves_Debug) {
			System.err.println(instances.toString(false));
		}

		if (instances == null)
			throw new Exception(" Unable to randomize the class orders.");
		instances.deleteWithMissingClass();
		if (instances.numInstances() == 0)
			throw new Exception(" No instances with a class value!");

		if (instances.numInstances() < m_Folds)
			throw new Exception(" Not enough data for REP.");

		m_Ruleset = new FastVector();
		m_RulesetStats = new FastVector();
		m_Distributions = new FastVector();

		// Get the class frequencies
		double[] orderedClasses = ((ModifiedInstances) instances).getClassCounts();
		if (m_Debug) {
			System.err.println("Sorted classes:");
			for (int x = 0; x < m_Class.numValues(); x++)
				System.err.println(x + ": " + m_Class.value(x) + " has "
						+ orderedClasses[x] + " instances.");
		}

		// Iterate from less prevalent class to more frequent one
		oneClass: for (int y = 0; y < instances.numClasses() - 1; y++) { // For each
			// class
			double classIndex = (double) y;
			if (m_Debug) {
				int ci = (int) classIndex;
				System.err.println("\n\nClass " + m_Class.value(ci) + "(" + ci + "): "
						+ orderedClasses[y]
						+ "instances\n=====================================\n");
			}

			if (Utils.eq(orderedClasses[y], 0.0)) // No data for this class
				continue oneClass;

			// The expected FP/err is the proportion of the class
			double all = 0;
			for (int i = y; i < orderedClasses.length; i++)
				all += orderedClasses[i];
			double expFPRate = orderedClasses[y] / all;

			double classYWeights = 0, totalWeights = 0;
			for (int j = 0; j < instances.numInstances(); j++) {
				Instance datum = instances.instance(j);
				// if(m_Debug)
				// System.err.println(datum + " w = "+datum.weight());
				totalWeights += datum.weight();
				if ((int) datum.classValue() == y) {
					classYWeights += datum.weight();
				}
			}

			// DL of default rule, no theory DL, only data DL
			double defDL;
			if (classYWeights > 0)
				defDL = StRuleStats.dataDL(expFPRate, 0.0, totalWeights, 0.0,
						classYWeights);
			else
				continue oneClass; // Subsumed by previous rules

			if (Double.isNaN(defDL) || Double.isInfinite(defDL))
				throw new Exception("Should never happen: defDL NaN or infinite!");
			if (m_Debug)
				System.err.println("The default DL = " + defDL);

			instances = rulesetForOneClass(expFPRate, instances, classIndex, defDL);
		}

		// Set the default rule
		RipperRule defRule = new RipperRule();
		defRule.setConsequent((double) (instances.numClasses() - 1));
		m_Ruleset.addElement(defRule);

		StRuleStats defRuleStat = new StRuleStats(m_Debug && ves_Debug);
		//defRuleStat.setData(instances);
		defRuleStat.setNumAllConds(m_Total);
		defRuleStat.addAndUpdate(defRule, instances);
		m_RulesetStats.addElement(defRuleStat);

		for (int z = 0; z < m_RulesetStats.size(); z++) {
			StRuleStats oneClass = (StRuleStats) m_RulesetStats.elementAt(z);
			for (int xyz = 0; xyz < oneClass.getRulesetSize(); xyz++) {
				double[] classDist = oneClass.getDistributions(xyz);
				Utils.normalize(classDist);
				if (classDist != null)
					m_Distributions.addElement(classDist);// ((ClassOrder)
				// m_Filter).distributionsByOriginalIndex(classDist));
			}
		}
	}

	/**
	 * Classify the test instance with the rule learner and provide the class
	 * distributions
	 * 
	 * @param datum
	 *          the instance to be classified
	 * @return the distribution
	 */
	public double[] distributionForInstance(Instance datum) {
		try {
			for (int i = 0; i < m_Ruleset.size(); i++) {
				RipperRule rule = (RipperRule) m_Ruleset.elementAt(i);
				if (rule.covers(datum))
					return (double[]) m_Distributions.elementAt(i);
			}
		} catch (Exception e) {
			System.err.println(e.getMessage());
			e.printStackTrace();
		}

		System.err.println("Should never happen!");
		return new double[datum.classAttribute().numValues()];
	}

	/**
	 * Build a ruleset for the given class according to the given data
	 * 
	 * @param expFPRate
	 *          the expected FP/(FP+FN) used in DL calculation
	 * @param data
	 *          the given data
	 * @param classIndex
	 *          the given class index
	 * @param defDL
	 *          the default DL in the data
	 * @exception if
	 *              the ruleset can be built properly
	 */
	protected Instances rulesetForOneClass(double expFPRate, Instances data,
			double classIndex, double defDL) throws Exception {

		Instances growData, pruneData;
		boolean stop = false;
		FastVector ruleset = new FastVector();

		double dl = defDL, minDL = defDL;
		StRuleStats rstats = null;
		double[] rst;

		// Check whether data have positive examples
		boolean defHasPositive = true; // No longer used
		boolean hasPositive = true;

		/** ******************** Building stage ********************** */
		if (m_Debug)
			System.err.println("\n*** Building stage ***");
		//data = StRuleStats.randomizeByDoc(data, m_Folds, m_Random);
		Instances[] part;
		while ((!stop) && hasPositive) { // Generate new rules until
			// stopping criteria met
			RipperRule oneRule;
			if (m_UsePruning) {
				/* Split data into Grow and Prune */

				// We should have stratified the data, but ripper seems
				// to have a bug that makes it not to do so. In order
				// to simulate it more precisely, we do the same thing.
				// newData.randomize(m_Random);
				/*
				 * 
				 * newData = StRuleStats.stratify(newData, m_Folds, m_Random);
				 * Instances[] part = StRuleStats.partition(newData, m_Folds); growData =
				 * part[0]; pruneData = part[1];
				 */
				//data = StRuleStats.randomizeByDoc(data, m_Folds, m_Random);
				//part = StRuleStats.partitionByDocument(data, m_Folds);
				part = StRuleStats.randomizeAndPartitionByDoc(data,m_Folds, m_Random);
				growData = part[0];
				pruneData = part[1];
				// growData=newData.trainCV(m_Folds, m_Folds-1);
				// pruneData=newData.testCV(m_Folds, m_Folds-1);

				oneRule = new RipperRule();
				oneRule.setConsequent(classIndex); // Must set first

				if (m_Debug)
					System.err.println("\nGrowing a rule ...");
				oneRule.grow(growData); // Build the rule
				if (m_Debug) {
					System.err.println("One rule found before pruning:" + oneRule.toString(m_Class));
					// throw new RuntimeException("breaking now");
				}
				if (m_Debug)
					System.err.println("\nPruning the rule ...");
				oneRule.prune(pruneData, false); // Prune the rule
				if (m_Debug)
					System.err.println("One rule found after pruning:" + oneRule.toString(m_Class));
			} else {
				oneRule = new RipperRule();
				oneRule.setConsequent(classIndex); // Must set first
				if (m_Debug)
					System.err.println("\nNo pruning: growing a rule ...");
				oneRule.grow(data); // Build the rule
				if (m_Debug)
					System.err.println("No pruning: one rule found:\n"
							+ oneRule.toString(m_Class));
			}

			// Compute the DL of this ruleset
			if (rstats == null) { // First rule
				rstats = new StRuleStats(m_Debug && ves_Debug);
				rstats.setNumAllConds(m_Total);
				//rstats.setData(newData);
			}
			rstats.addAndUpdate(oneRule, data);
			int last = rstats.getRuleset().size() - 1; // Index of last rule
			dl += rstats.relativeDL(data, last, expFPRate, m_CheckErr);

			if (Double.isNaN(dl) || Double.isInfinite(dl))
				throw new Exception(
						"Should never happen: dl in building stage NaN or infinite!");
			if (m_Debug)
				System.err.println("Before optimization(" + last + "): the dl = " + dl
						+ " | best: " + minDL);

			if (dl < minDL)
				minDL = dl; // The best dl so far

			rst = rstats.getSimpleStats(last);
			if (m_Debug)
				System.err.println("The rule covers: " + rst[0] + " | pos = " + rst[2]
						+ " | neg = " + rst[4] + "\nThe rule doesn't cover: " + rst[1]
						+ " | pos = " + rst[5]);

			stop = checkStop(rst, minDL, dl);

			if (!stop) {
				ruleset.addElement(oneRule); // Accepted
				// newData = rstats.getFiltered(last)[1];// Data not covered
				data = oneRule.apply(ruleset, data);			
				hasPositive = Utils.gr(rst[5], 0.0); // Positives remaining?
				if (m_Debug)
					System.err.println("One rule added: has positive? " + hasPositive);
			} else {
				if (m_Debug)
					System.err.println("Quit rule");
				rstats.removeLast(); // Remove last to be re-used
			}
		}// while !stop
		
		System.out.println("Ruleset (in Ripper format) after building stage \n"+rstats.toRIPPERString());
		/** ****************** Optimization stage ****************** */
		StRuleStats finalRulesetStat = null;
		if (m_UsePruning) {
			for (int z = 0; z < m_Optimizations; z++) {
				if (m_Debug)
					System.err.println("\n*** Optimization: run #" + z + " ***");

				// newData = data;
				finalRulesetStat = new StRuleStats(m_Debug && ves_Debug);
				//finalRulesetStat.setData(newData);
				finalRulesetStat.setNumAllConds(m_Total);
				(new RipperRule()).clear(data,false);				
				int position = 0;
				stop = false;
				boolean isResidual = false;
				hasPositive = defHasPositive;
				dl = minDL = defDL;

				oneRule: while (!stop && hasPositive) {

					isResidual = (position >= ruleset.size()); // Cover residual positive
					// examples
					// Re-do shuffling and stratification
					// newData.randomize(m_Random);
					//data = StRuleStats.randomizeByDoc(data, m_Folds, m_Random);
					//Instances[] part = StRuleStats.partitionByDocument(data, m_Folds);
					/*Instances[]*/ 
					part = StRuleStats.randomizeAndPartitionByDoc(data, m_Folds, m_Random);
					growData = part[0];
					pruneData = part[1];
					// growData=newData.trainCV(m_Folds, m_Folds-1);
					// pruneData=newData.testCV(m_Folds, m_Folds-1);
					RipperRule finalRule;

					if (m_Debug)
						System.err.println("\nRule #" + position + "| isResidual?"
								+ isResidual + "| data size: " + data.sumOfWeights());

					if (isResidual) {
						RipperRule newRule = new RipperRule();
						newRule.setConsequent(classIndex);
						if (m_Debug)
							System.err.println("\nGrowing and pruning" + " a new rule ...");
						newRule.grow(growData);
						newRule.prune(pruneData, false);
						finalRule = newRule;
						if (m_Debug)
							System.err.println("\nNew rule found: "
									+ newRule.toString(m_Class));
					} else {
						RipperRule oldRule = (RipperRule) ruleset.elementAt(position);
						boolean covers = false;
						// Test coverage of the next old rule
						for (int i = 0; i < data.numInstances() && !covers; i++)
							if (oldRule.covers(data.instance(i))) {
								covers = true;
							}

						if (!covers) {// Null coverage, no variants can be generated
							if(m_Debug)
								System.err.println("Rule "+oldRule+" has 0 coverage. Adding.");
							finalRulesetStat.addAndUpdate(oldRule, data);
							position++;
							continue oneRule;
						}

						// 2 variants
						if (m_Debug)
							System.err.println("\nGrowing and pruning" + " Replace ...");
						RipperRule replace = new RipperRule();
						replace.setConsequent(classIndex);
						replace.applyWhileSkipping(ruleset, growData, position);
						transitiveClosure(growData, m_Covered);
						replace.grow(growData);
						if (m_Debug)
							System.err.println("\nReplace rule "+replace);
						
						// Remove the pruning data covered by the following
						// rules, then simply compute the error rate of the
						// current rule to prune it. According to Ripper,
						// it's equivalent to computing the error of the
						// whole ruleset -- is it true?
						// TO DO: This has to be modified
						//pruneData = StRuleStats.rmCoveredBySuccessives(pruneData, ruleset, position);
						//Apply and transitive close all rules in the ruleset except for the current rule
						replace.applyWhileSkipping(ruleset, pruneData, position);
						transitiveClosure(pruneData, m_Covered);
						
						replace.pruneWithRuleset(pruneData, true);

						if (m_Debug)
							System.err.println("\nGrowing and pruning" + " Revision ...");
						RipperRule revision = (RipperRule) oldRule.copy();
	
						// For revision, first rm the data covered by the old rule
						/* We use transitive closure here */
						//ModifiedInstances newGrowData = new ModifiedInstances(growData, 0);
						//revision.apply(growData, false);
						//transitiveClosure(growData, m_PredictedClass);
						//int positive = m_PredictedClass.indexOfValue("+");
						//for (int b = 0; b < growData.numInstances(); b++) {
						//	Instance inst = growData.instance(b);
						//	if ((int)inst.value(m_PredictedClass)==positive)
						//		newGrowData.add(inst);
						//}
						revision.apply(ruleset,growData,position);
						revision.grow(growData);
						if (m_Debug)
							System.err.println("\nRevision rule "+revision);
						revision.prune(pruneData, true);

						double[][] prevRuleStats = new double[position][6];
						for (int c = 0; c < position; c++)
							prevRuleStats[c] = finalRulesetStat.getSimpleStats(c);

						// Now compare the relative DL of variants
						FastVector tempRules = (FastVector) ruleset.copyElements();
						tempRules.setElementAt(replace, position);
						//revision.clear(data,false);
						
						StRuleStats repStat = new StRuleStats(data, tempRules, m_Debug && ves_Debug);
						repStat.setNumAllConds(m_Total);
						repStat.countData(position, data, prevRuleStats);
						// repStat.countData();
						rst = repStat.getSimpleStats(position);
						if (m_Debug)
							System.err.println("Replace rule covers: " + rst[0] + " | pos = "
									+ rst[2] + " | neg = " + rst[4]
									+ "\nThe rule doesn't cover: " + rst[1] + " | pos = " + rst[5]);

						double repDL = repStat.relativeDL(data, position, expFPRate, m_CheckErr);
						if (m_Debug)
							System.err.println("\nReplace: " + replace.toString(m_Class)
									+ " |dl = " + repDL);

						if (Double.isNaN(repDL) || Double.isInfinite(repDL))
							throw new Exception("Should never happen: repDL in optmz. stage NaN or infinite!");

						tempRules.setElementAt(revision, position);
						StRuleStats revStat = new StRuleStats(data, tempRules, m_Debug
								&& ves_Debug);
						revStat.setNumAllConds(m_Total);
						revStat.countData(position, data, prevRuleStats);
						// revStat.countData();
						double revDL = revStat.relativeDL(data, position, expFPRate, m_CheckErr);

						if (m_Debug)
							System.err.println("Revision: " + revision.toString(m_Class)+ " |dl = " + revDL);

						if (Double.isNaN(revDL) || Double.isInfinite(revDL))
							throw new Exception("Should never happen: revDL in optmz. stage NaN or infinite!");

						rstats = new StRuleStats(data, ruleset, m_Debug && ves_Debug);
						rstats.setNumAllConds(m_Total);
						rstats.countData(position, data, prevRuleStats);
						// rstats.countData();
						double oldDL = rstats.relativeDL(data, position, expFPRate, m_CheckErr);

						if (Double.isNaN(oldDL) || Double.isInfinite(oldDL))
							throw new Exception("Should never happen: oldDL in optmz. stage NaN or infinite!");
						if (m_Debug)
							System.err.println("Old rule: " + oldRule.toString(m_Class)+ " |dl = " + oldDL);

						if (m_Debug)
							System.err.println("\nrepDL: " + repDL + "\nrevDL: " + revDL
									+ "\noldDL: " + oldDL);

						if ((oldDL <= revDL) && (oldDL <= repDL))
							finalRule = oldRule; // Old the best
						else if (revDL <= repDL)
							finalRule = revision; // Revision the best
						else
							finalRule = replace; // Replace the best
					}

					finalRulesetStat.addAndUpdate(finalRule, data);
					rst = finalRulesetStat.getSimpleStats(position);

					if (isResidual) {

						dl += finalRulesetStat.relativeDL(data, position, expFPRate, m_CheckErr);
						if (m_Debug)
							System.err.println("After optimization: the dl" + "=" + dl + " | best: " + minDL);

						if (dl < minDL)
							minDL = dl; // The best dl so far

						stop = checkStop(rst, minDL, dl);
						if (!stop){
							ruleset.addElement(finalRule); // Accepted
						}else {
							finalRulesetStat.removeLast(); // Remove last to be re-used
							position--;
						}
					} else
						ruleset.setElementAt(finalRule, position); // Accepted
					//(new RipperRule()).apply(ruleset,data,position+1);
					//transitiveClosure(data, m_Covered);
					if (m_Debug) {
						System.err.println("The rule covers: " + rst[0] + " | pos = "
								+ rst[2] + " | neg = " + rst[4] + "\nThe rule doesn't cover: "
								+ rst[1] + " | pos = " + rst[5]);
						System.err.println("\nRuleset so far: ");
						for (int x = 0; x < ruleset.size(); x++)
							System.err.println(x + ": "
									+ ((RipperRule) ruleset.elementAt(x)).toString(m_Class));
						System.err.println();
					}

					// Data not covered
					//if (finalRulesetStat.getRulesetSize() > 0)// If any rules
					//	newData = finalRulesetStat.getFiltered(position)[1];
					
					hasPositive = Utils.gr(rst[5], 0.0); // Positives remaining?
					position++;
				} // while !stop && hasPositive

				if (ruleset.size() > (position + 1)) { // Hasn't gone through yet
					for (int k = position + 1; k < ruleset.size(); k++)
						finalRulesetStat.addAndUpdate((Rule) ruleset.elementAt(k), data);
				}
				if (m_Debug)
					System.err
							.println("\nDeleting rules to decrease DL of the whole ruleset ...");
				if(m_Debug){
					System.err.println("Reducing length of "+finalRulesetStat);
				}
					
				finalRulesetStat.reduceDL(data, expFPRate, m_CheckErr);
				if (m_Debug) {
					int del = ruleset.size() - finalRulesetStat.getRulesetSize();
					System.err.println(del
							+ " rules are deleted after DL reduction procedure");
				}
				ruleset = finalRulesetStat.getRuleset();
				rstats = finalRulesetStat;
				System.out.println("Ruleset (in Ripper format) after optimization run "+z+"\n"
						+rstats.toRIPPERString());
			} // For each run of optimization
		} // if pruning is used

		// Concatenate the ruleset for this class to the whole ruleset
		if (m_Debug) {
			System.err.println("\nFinal ruleset (size " + ruleset.size() + "): ");
			for (int x = 0; x < ruleset.size(); x++)
				System.err.println(x + ": "
						+ ((RipperRule) ruleset.elementAt(x)).toString(m_Class));
			System.err.println();
		}

		m_Ruleset.appendElements(ruleset);
		m_RulesetStats.addElement(rstats);

		//if (ruleset.size() > 0)// If any rules for this class
		//	return rstats.getFiltered(ruleset.size() - 1)[1]; // Data not covered
		//else
			return data;
	}

	/**
	 * Check whether the stopping criterion meets
	 * 
	 * @param rst
	 *          the statistic of the ruleset
	 * @param minDL
	 *          the min description length so far
	 * @param dl
	 *          the current description length of the ruleset
	 * @return true if stop criterion meets, false otherwise
	 */
	private boolean checkStop(double[] rst, double minDL, double dl) {

		if (dl > minDL + MAX_DL_SURPLUS) {
			if (m_Debug)
				System.err.println("DL too large: " + dl + " | " + minDL);
			return true;
		} else if (!Utils.gr(rst[2], 0.0)) {// Covered positives
			if (m_Debug)
				System.err.println("Too few positives.");
			return true;
		} else if ((rst[4] / rst[0]) >= 0.5) {// Err rate
			if (m_CheckErr) {
				if (m_Debug)
					System.err.println("Error too large: " + rst[4] + "/" + rst[0]);
				return true;
			} else
				return false;
		} else {// Not stops
			if (m_Debug)
				System.err.println("Continue.");
			return false;
		}
	}

	/**
	 * Prints the all the rules of the rule learner.
	 * 
	 * @return a textual description of the classifier
	 */
	public String toString() {
		if (m_Ruleset == null)
			return "StRIP: No model built yet.";

		StringBuffer sb = new StringBuffer("StRip rules:\n" + "===========\n\n");
		for (int j = 0; j < m_RulesetStats.size(); j++) {
			StRuleStats rs = (StRuleStats) m_RulesetStats.elementAt(j);
			FastVector rules = rs.getRuleset();
			for (int k = 0; k < rules.size(); k++) {
				double[] simStats = rs.getSimpleStats(k);
				sb.append(((RipperRule) rules.elementAt(k)).toString(m_Class) + " ("
						+ simStats[0] + "/" + simStats[4] + ")\n");
			}
		}
		if (m_Debug) {
			System.err.println("Inside m_Ruleset");
			for (int i = 0; i < m_Ruleset.size(); i++)
				System.err.println(((RipperRule) m_Ruleset.elementAt(i))
						.toString(m_Class));
		}
		sb.append("\nNumber of Rules : " + m_Ruleset.size() + "\n");
		return sb.toString();
	}

	/**
	 * Main method.
	 * 
	 * @param args
	 *          the options for the classifier
	 */
	public static void main(String[] args) {
		try {
			System.out.println(Evaluation.evaluateModel(new StRip(), args));
		} catch (Exception e) {
			e.printStackTrace();
			System.err.println(e.getMessage());
		}
	}

	public static Instances transitiveClosure(Instances data,
			Attribute closureAttribute) {
		ModifiedInstances result = (ModifiedInstances) data;
		Attribute DOCID = result.attribute("DOCNUM");
		Attribute ID1 = result.attribute("ID1");
		Attribute ID2 = result.attribute("ID2");
		Attribute CLASS = closureAttribute;
		Attribute covered = result.attribute("covered");
		boolean otherClass = !CLASS.equals(covered); 
		int positive = CLASS.indexOfValue("+");
		int startPositive = 0, endPositive = 0, startPositiveTotal = 0, endPositiveTotal = 0;
		// Compute the transitive closure of the real_class attribute
		int docnum = -1;
		int[] ptrs = null;
		int start = -1;
		for (int i = 0; i < result.numInstances(); i++) {
			// read all instances for a document
			Instance current = result.instance(i);
			// System.out.println("Working on: "+current);
			int currentDoc = (int) current.value(DOCID);
			int id1 = (int) current.value(ID1);
			int id2 = (int) current.value(ID2);
			boolean cl = (int) current.value(CLASS)==positive;
			double cov;
			boolean notCovered;
			
			if (docnum != currentDoc && docnum != -1) {
				// A new document. Finish up the last document.
				// need to loop over all the entries for the document and update the
				// class value.
				for (int k = start; k < i; k++) {
					Instance curPair = result.instance(k);
					int curId1 = (int) curPair.value(ID1);
					int curId2 = (int) curPair.value(ID2);
					cov = curPair.value(covered);
					notCovered = Double.isNaN(cov) || (int) cov != positive;
					notCovered = notCovered && otherClass;
					if (find(curId1, ptrs) == find(curId2, ptrs)){
						curPair.setValue(CLASS, "+");
						if(notCovered){
							endPositive+=curPair.weight();
							endPositiveTotal++;
						}
					}
				}
			}
			if (docnum == -1 || docnum != currentDoc) {
				// A new document document. Some setting up to be done
				int len = id2 + 1;
				ptrs = new int[len];
				// initialize pointers so each item is in it's own set
				for (int j = 0; j < len; j++) {
					ptrs[j] = j;
				}
				start = i;
				docnum = currentDoc;
			}
			if (cl) {
				union(id1, id2, ptrs);
				cov  = current.value(covered);
				notCovered = Double.isNaN(cov) || (int) cov != positive;
				notCovered = notCovered && otherClass;
				if (notCovered) {
					startPositive += current.weight();
					startPositiveTotal++;
				}
			}
		}
		for (int k = start; k < result.numInstances(); k++) {
			Instance curPair = result.instance(k);
			int curId1 = (int) curPair.value(ID1);
			int curId2 = (int) curPair.value(ID2);
			double cov = curPair.value(covered);
			boolean notCovered = Double.isNaN(cov) || (int) cov != positive;
			notCovered = notCovered && otherClass;
			// System.out.println(curId1+"-"+find(curId1,ptrs)+" and
			// "+curId2+"-"+find(curId2,ptrs));
			if (find(curId1, ptrs) == find(curId2, ptrs)){
				curPair.setValue(CLASS, "+");
				if(notCovered){
					endPositive+=curPair.weight();
					endPositiveTotal++;
				}
			}						
		}
		//System.err.print("[TC:"+startPositive+"("+startPositiveTotal+")->"
		//		+endPositive+"("+endPositiveTotal+")]");
		return result;
	}

	private static int find(int i, int[] ptrs) {
		// find the set number for the element
		int ind = i;
		while (ind != ptrs[ind]) {
			ind = ptrs[ind];
		}
		// fix the link so that it is one hop only
		// note: this doesn't implement the full union-find update
		ptrs[i] = ind;
		return ind;
	}

	private static void union(int i, int j, int[] ptrs) {
		int indI = find(i, ptrs);
		int indJ = find(j, ptrs);
		ptrs[indI] = indJ;
	}
}